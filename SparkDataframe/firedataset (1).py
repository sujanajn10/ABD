# -*- coding: utf-8 -*-
"""firedataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14XdOsT4FORykll5oc0h42bbYeAId5Kwl
"""

!pip install pyspark

import pyspark
from pyspark.sql import SparkSession
spark=SparkSession.builder.appName('Fire DF').getOrCreate()

df=spark.read.csv("drive/My Drive/Colab Notebooks/sf-fire-calls.csv",header=True,inferSchema=True)
df.show()
df.printSchema()



df1=df.select('CallType','CallDate','City','Zipcode','Neighborhood','Delay')
df1.show()

df1.withColumn('Date',to_date(col('Calldate'),'MM/dd/yyyy')).show()

df2=df1.withColumn('Date',to_date(col('Calldate'),'MM/dd/yyyy')).drop('CallDate')

df2.show()

df3=df2.withColumn('year',year(col('Date')))\
  .withColumn('Month',month(col('Date')))\
  .withColumn('week',weekofyear(col('Date')))

df3.show()
df3.printSchema()

#create a user defined function
def mapSession(data):
  if 2 < data < 6:
    return 'spring'
  elif 5 < data < 9:
    return 'summer'
  elif 8 < data < 12:
    return 'autumn'
  else:
      return 'winter'

seasonUDF=udf(mapSession,StringType())
clean_udf=df3.withColumn('Season',seasonUDF(col('Month')))
clean_udf.show()

clean_udf.select('CallType','Date','City','Month','Season')\
  .where(col('Season') == 'spring').show()

clean_udf.select('Year').groupBy('Year').count().orderBy('Year',ascending=True).show()

import pandas as pd
import matplotlib.pyplot as plt
panda_df=clean_udf.select('Year')\
         .groupBy('Year').count()\
         .orderBy('Year',ascending=True).toPandas()
panda_df.plot.line(x='Year',y='count')
plt.show()

clean_udf.select('CallType').where(col('Year') == 2018).distinct().show(truncate=False)

from pyspark.sql.window import Window
season_count=clean_udf.select('CallType','Season')\
  .filter(col('Year')==2014)\
  .groupBy('Season','CallType')\
  .count()\
  .withColumn('Rank',dense_rank().over(Window.partitionBy('Season').orderBy(col('count').desc())))
season_count.show()

#question2
clean_udf.select("CallType","Year")\
         .where(col("Year")==2018).distinct().show(truncate=False)

#question3
clean_udf.select('Week')\
.where(col('Year')==2018)\
.groupBy('Week')\
.count()\
.orderBy("count",ascending=False).collect()[0][0]

#question3
clean_udf.select("Month")\
.where(col('Year')==2018)\
.groupBy('Month')\
.count()\
.orderBy("count",asecending=False).collect()[0][0]

#question4
clean_udf.select("Month")\
.orderBy('CallType')\
.groupBy('Month')\
.count()\
.orderBy("count",asecending=False).collect()[0][0]

#question5
selected_year = 2018
clean_ud= clean_udf.filter(clean_udf["Year"] == selected_year)
monthly_report = clean_ud.groupBy("Year", "Month", "CallType")\
.agg(count(col("CallType")).alias("call_count"))
monthly_report.show()

#question6
from pyspark.sql.window import Window
season_count=clean_udf.select("CallType","Season")\
.filter(col('Year')==2014)\
.groupBy('Season','CallType')\
.count()\
.withColumn('rank',dense_rank().over(Window.partitionBy('Season').orderBy(col('count').desc())))\
.orderBy('Season','rank',ascending=[1,1])\
.filter(col('rank')<6)
season_count.show()

#question7
fire_calls = clean_udf.filter(clean_udf["CallType"] == "Structure Fire")

monthly_counts = fire_calls.groupBy(date_format("Date", "yyyy-MM").alias("Month")).count()

monthly_counts.show()

#question8
fire=clean_udf.filter(clean_udf["Year"]==2018)\
.groupBy("Month").count()\
.orderBy('count',ascending=False).show()

#question9
fire_calls = clean_udf.filter(clean_udf["CallType"] == "Structure Fire")
year = fire_calls.groupBy(year("Date").alias("Year"), "CallType").count()
major_call_per_year = year.orderBy("Year", desc("count")) \
    .groupBy("Year").agg(first("CallType").alias("MajorCallType"))
major_call_per_year.show()

#question10
average_delay = clean_udf.groupBy("CallType").agg(avg("Delay").alias("AverageDelay"))
average_delay.show()

#question11
max_avg_delay = average_delay.orderBy(desc("AverageDelay")).first()
#max_avg_delay.show()
print(max_avg_delay)

#question12

#question13
clean_udf.select("Neighborhood", "Delay").filter(year("IncidentNumber") == 2018).show()

#question14
fire1=clean_udf.filter(clean_udf["CallType"] == "Structure Fire")
result=fire.groupBy("Year", "CallType").agg(avg("Delay").alias("AverageDelay"))
maxdelay=result.groupBy("CallType").agg({"AverageDelay": "max"})
maxdelay.show()

#question15
fire = clean_udf.filter(clean_udf["CallType"] == "Structure Fire")
city= fire.groupBy("Year", "City").count()
window_spec = Window.partitionBy("Year").orderBy(city["count"].desc())
max_city = first(city["City"]).over(window_spec)
result = city.withColumn("Max_Call_City", max_city)
result.select("Year", "Max_Call_City").distinct().show()

#question16

year_city= clean_udf.groupBy("Year", "City", "CallType") \
    .agg(count("*").alias("CallTypeCount"))
window = Window.partitionBy("Year").orderBy(desc("CallTypeCount"))
cities = year_city.withColumn("rank", rank().over(window))\
.filter(col("rank") <= 5)
cities.show()

#question17

correlation = clean_udf.filter(df["CallType"] == "Structure Fire").groupBy("Neighborhood", "ZipCode").count()\
    .select(corr("ZipCode", "count")).collect()[0][0]
correlation